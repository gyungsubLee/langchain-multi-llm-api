#!/usr/bin/env python3
"""
LangChain Multi-LLM API ν…μ¤νΈ μ¤ν¬λ¦½νΈ
v1, v2 μ—”λ“ν¬μΈνΈ μλ™ ν…μ¤νΈ
"""
import httpx
import subprocess
import time
import sys
import os
import signal

BASE_URL = "http://127.0.0.1:8000"
UVICORN_CMD = ["uvicorn", "app.main:app", "--host", "127.0.0.1", "--port", "8000", "--log-level", "info"]


def wait_for_server(max_attempts=30):
    """μ„λ²„κ°€ μ¤€λΉ„λ  λ•κΉμ§€ λ€κΈ°"""
    client = httpx.Client()
    for _ in range(max_attempts):
        try:
            resp = client.get(BASE_URL)
            if resp.status_code == 200:
                return True
        except httpx.ConnectError:
            time.sleep(0.5)
    return False


def test_v1_endpoints():
    """v1 μ—”λ“ν¬μΈνΈ ν…μ¤νΈ (κΈ°λ³Έ LLM νΈμ¶)"""
    print("\n" + "=" * 60)
    print("V1 μ—”λ“ν¬μΈνΈ ν…μ¤νΈ (κΈ°λ³Έ LLM)")
    print("=" * 60)

    client = httpx.Client(timeout=20.0)

    v1_tests = [
        ("/v1/gpt", {"prompt": "μ•λ…•, gpt ν…μ¤νΈν•΄μ¤"}),
        ("/v1/gemini", {"prompt": "μ•λ…•, gemini ν…μ¤νΈν•΄μ¤"}),
        ("/v1/claude", {"prompt": "μ•λ…•, claude ν…μ¤νΈν•΄μ¤"}),
    ]

    for endpoint, payload in v1_tests:
        url = f"{BASE_URL}{endpoint}"
        print(f"\n-> POST {url}")
        print(f"   payload={payload}")

        try:
            resp = client.post(url, json=payload)
            print(f"   Response ({resp.status_code}): {resp.json()}\n")
        except Exception as e:
            print(f"   Error: {e}\n")


def test_v2_endpoints():
    """v2 μ—”λ“ν¬μΈνΈ ν…μ¤νΈ (Prompt Template)"""
    print("\n" + "=" * 60)
    print("V2 μ—”λ“ν¬μΈνΈ ν…μ¤νΈ (Prompt Template)")
    print("=" * 60)

    client = httpx.Client(timeout=20.0)

    v2_tests = [
        ("/v2/prompt-template", {"text": "μ•λ…•", "target_lang": "μμ–΄"}),
        ("/v2/chat-prompt-template", {"text": "μΆ‹μ€ μ•„μΉ¨", "system_message": "μ‚¬μ©μμ μ§μλ¥Ό μΌλ³Έμ–΄λ΅ λ²μ—­ν•΄λΌ."}),
        ("/v2/translate", {"text": "κ³ λ§μ›", "target_lang": "μ¤‘κµ­μ–΄"}),
    ]

    for endpoint, payload in v2_tests:
        url = f"{BASE_URL}{endpoint}"
        print(f"\n-> POST {url}")
        print(f"   payload={payload}")

        try:
            resp = client.post(url, json=payload)
            print(f"   Response ({resp.status_code}):")
            response_data = resp.json()
            for key, value in response_data.items():
                print(f"     {key}: {value}")
            print()
        except Exception as e:
            print(f"   Error: {e}\n")


def main():
    """λ©”μΈ μ‹¤ν–‰ ν•¨μ"""
    print("=" * 60)
    print("LangChain Multi-LLM API ν…μ¤νΈ μ‹μ‘")
    print("=" * 60)

    # ν™κ²½λ³€μ μ„¤μ •
    env = os.environ.copy()
    env.setdefault("MOCK", "true")
    mock_mode = env.get("MOCK", "true").lower() == "true"
    print(f"π”§ MOCK Mode: {mock_mode}\n")

    # uvicorn μ„λ²„ μ‹μ‘
    print("Starting uvicorn server...")
    server_process = subprocess.Popen(UVICORN_CMD, env=env)

    # μ„λ²„ μ¤€λΉ„ λ€κΈ°
    if not wait_for_server():
        print("β Server failed to start")
        server_process.terminate()
        sys.exit(1)

    print("β… Server ready β€” running endpoint tests\n")

    try:
        # v1 μ—”λ“ν¬μΈνΈ ν…μ¤νΈ
        test_v1_endpoints()

        # v2 μ—”λ“ν¬μΈνΈ ν…μ¤νΈ
        test_v2_endpoints()

        print("\n" + "=" * 60)
        print("β… λ¨λ“  ν…μ¤νΈ μ™„λ£!")
        print("=" * 60)

    finally:
        # μ„λ²„ μΆ…λ£
        print("\nStopping server...")
        try:
            server_process.send_signal(signal.SIGINT)
            server_process.wait(timeout=5)
        except Exception:
            server_process.kill()


if __name__ == "__main__":
    main()
